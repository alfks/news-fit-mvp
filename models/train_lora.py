import os
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer, SFTConfig

# ==========================================
# 1. ì„¤ì • (Configuration)
# ==========================================
PERSONA_TYPE = "conservative"  # 'conservative' ë˜ëŠ” 'progressive'
MODEL_ID = "Qwen/Qwen2.5-1.5B-Instruct"
DATA_PATH = f"./data/raw/raw_text_for_lora/train_{PERSONA_TYPE}.txt"
OUTPUT_DIR = f"./models/adapter_{PERSONA_TYPE}"

# GPU í™•ì¸
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ðŸ”¹ ì‹¤í–‰ ìž¥ì¹˜: {device.upper()}")

# ==========================================
# 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ==========================================
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_PATH}")

with open(DATA_PATH, "r", encoding="utf-8") as f:
    lines = [line.strip() for line in f.read().split('\n') if len(line) > 10]

print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(lines)} ë¬¸ìž¥")

alpaca_prompt = """### ì§€ì‹œ:
ë‹¹ì‹ ì€ {persona} ì„±í–¥ì˜ ë‰´ìŠ¤ ì—ë””í„°ìž…ë‹ˆë‹¤. ì´ ë¬¸ìž¥ì„ {persona} ê´€ì ì˜ ì‚¬ì„¤ì¡°ë¡œ ìž¬ìž‘ì„±í•˜ì„¸ìš”.

### ìž…ë ¥:
{}

### ì‘ë‹µ:
{}"""

def formatting_prompts_func(examples):
    instruction = "ë³´ìˆ˜" if PERSONA_TYPE == "conservative" else "ì§„ë³´"
    texts = []
    for output in examples["text"]:
        # ìž…ë ¥(Input)ì€ ë¹„ì›Œë‘ê³ ("{}"), ì‹¤ì œ ì‚¬ì„¤ ë‚´ìš©(output)ì„ í•™ìŠµ
        text = alpaca_prompt.format("{}", output, persona=instruction)
        texts.append(text)
    return { "text" : texts }

dataset = Dataset.from_dict({"text": lines})
dataset = dataset.map(formatting_prompts_func, batched=True)

# ==========================================
# 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# ==========================================
print("â³ ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
tokenizer.pad_token = tokenizer.eos_token 

bnb_config = None
if device == "cuda":
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config if device == "cuda" else None,
    device_map={"": "cuda"} if device == "cuda" else "cpu", # GPU ê°•ì œ í• ë‹¹
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    low_cpu_mem_usage=True
)

# ==========================================
# 4. LoRA ì„¤ì •
# ==========================================
peft_config = LoraConfig(
    r=16,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)
model = get_peft_model(model, peft_config)

# ==========================================
# 5. í•™ìŠµ ì‹¤í–‰ (Trainer)
# ==========================================
# [í•µì‹¬ ìˆ˜ì •] ìµœì‹  trl ë¬¸ë²• ì ìš©
sft_config = SFTConfig(
    output_dir=OUTPUT_DIR,
    dataset_text_field="text",
    max_length=512,         
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    logging_steps=5,
    max_steps=30,
    save_steps=50,
    fp16=(device == "cuda"),
    use_cpu=(device == "cpu"),
    report_to="none",
    packing=False,
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    processing_class=tokenizer, 
    args=sft_config,    
)

print(f"ðŸš€ í•™ìŠµ ì‹œìž‘ ({PERSONA_TYPE} ì„±í–¥)...")
trainer.train()

# ==========================================
# 6. ì €ìž¥
# ==========================================
print(f"ðŸ’¾ ëª¨ë¸ ì €ìž¥ ì¤‘... -> {OUTPUT_DIR}")
trainer.model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("ðŸŽ‰ í•™ìŠµ ì™„ë£Œ!")