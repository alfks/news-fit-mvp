# src/generator.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

class StyleGenerator:
    def __init__(self, base_model_id="Qwen/Qwen2.5-1.5B-Instruct"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_id)
        self.base_model = AutoModelForCausalLM.from_pretrained(
            base_model_id,
            torch_dtype=torch.float16,
            device_map="auto",
            offload_folder="./offload", # [í•µì‹¬] ë¶€ì¡±í•œ ë©”ëª¨ë¦¬ë¥¼ í•˜ë“œë””ìŠ¤í¬ë¡œ ëŒ€ì²´
            low_cpu_mem_usage=True
        )
        self.active_adapter = None

    def load_adapter(self, bias_type):
        """ì‚¬ìš©ì ì„±í–¥(bias_type)ì— ë”°ë¼ LoRA ì–´ëŒ‘í„° Hot-Swapping"""
        adapter_path = f"./models/adapter_{bias_type}"
        
        if self.active_adapter != bias_type:
            print(f"ğŸ”„ Switching Adapter to: {bias_type}")
            # ê¸°ì¡´ ì–´ëŒ‘í„° í•´ì œ í›„ ìƒˆ ì–´ëŒ‘í„° ë³‘í•© ë¡œì§ (PeftModel í™œìš©)
            self.model = PeftModel.from_pretrained(self.base_model, adapter_path)
            self.active_adapter = bias_type

    def generate(self, original_text, context_data, target_persona):
        """í”„ë¡¬í”„íŠ¸ ì¡°ë¦½ ë° ìƒì„±"""
        
        fact = context_data['fact_anchor']
        trojan = context_data['trojan_horse']
        
        prompt = f"""
### ì—­í• :
ë‹¹ì‹ ì€ {target_persona} ì„±í–¥ì˜ ë‰´ìŠ¤ ì—ë””í„°ì…ë‹ˆë‹¤. 
ë…ìê°€ ì½ê¸° í¸í•˜ë„ë¡ ì•„ë˜ ì§€ì¹¨ì— ë”°ë¼ ê¸°ì‚¬ë¥¼ ì¬ì‘ì„±í•˜ì„¸ìš”.

### ì§€ì¹¨:
1. [íŒ©íŠ¸]ëŠ” ì ˆëŒ€ ì™œê³¡í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ìœ ì§€í•  ê²ƒ. (Fact Anchoring)
2. [íŠ¸ë¡œì´ ëª©ë§ˆ] ë‚´ìš©ì„ ê¸°ì‚¬ ì¤‘ê°„ì— "ì¼ê°ì—ì„œëŠ” ~ë¼ëŠ” ì˜ê²¬ë„ ìˆë‹¤" í˜•íƒœë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš©í•  ê²ƒ.
3. ì „ì²´ì ì¸ ì–´ì¡°ëŠ” {target_persona} ìŠ¤íƒ€ì¼ì— ë§ì¶œ ê²ƒ.

### ì…ë ¥ ë°ì´í„°:
- ì›ë¬¸: {original_text}
- [íŒ©íŠ¸]: {fact}
- [íŠ¸ë¡œì´ ëª©ë§ˆ]: {trojan}

### ì¬ì‘ì„±ëœ ê¸°ì‚¬:
"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.model.generate(**inputs, max_new_tokens=600)
            
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)