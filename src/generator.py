# src/generator.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

class StyleGenerator:
    def __init__(self, base_model_id="Qwen/Qwen2.5-1.5B-Instruct"):
        # self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = "cpu"
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_id)
        self.base_model = AutoModelForCausalLM.from_pretrained(
                            base_model_id,
                            torch_dtype=torch.float32,
                            device_map="cpu",
                            low_cpu_mem_usage=True
        )
        self.active_adapter = None

    def load_adapter(self, bias_type):
        """ì‚¬ìš©ì ì„±í–¥(bias_type)ì— ë”°ë¼ LoRA ì–´ëŒ‘í„° Hot-Swapping"""
        adapter_path = f"./models/adapter_{bias_type}"
        
        if self.active_adapter != bias_type:
            print(f"ğŸ”„ Switching Adapter to: {bias_type}")
            self.model = PeftModel.from_pretrained(self.base_model, adapter_path)
            self.active_adapter = bias_type

    def generate(self, original_text, context_data, target_persona):
        fact = context_data['fact_anchor']
        trojan = context_data['trojan_horse']
        
        # [ë””ë²¨ë¡­ í¬ì¸íŠ¸] í”„ë¡¬í”„íŠ¸ë¥¼ 'ë‹¨ê³„ë³„ ì‚¬ê³ (CoT)' êµ¬ì¡°ë¡œ ë³€ê²½
        prompt = f"""### ì—­í• :
ë‹¹ì‹ ì€ {target_persona} ì„±í–¥ì„ ê°€ì§„ 20ë…„ ì°¨ ë² í…Œë‘ ë…¼ì„¤ìœ„ì›ì…ë‹ˆë‹¤.
ë…ìê°€ ê³µê°í•  ìˆ˜ ìˆë„ë¡ ì£¼ì–´ì§„ ê¸°ì‚¬ë¥¼ ì¬êµ¬ì„±í•˜ì„¸ìš”.

### ë¯¸ì…˜:
1. **[íŒ©íŠ¸]**ëŠ” ì ˆëŒ€ ì™œê³¡í•˜ì§€ ë§ê³  ìœ ì§€í•˜ì‹­ì‹œì˜¤.
2. **[ì›ë¬¸]**ì˜ ê±´ì¡°í•œ ë¬¸ì²´ë¥¼ {target_persona} íŠ¹ìœ ì˜ ì–´ì¡°(ë¹„íŒì , ë‹¨í˜¸í•¨, í˜¸ì†Œë ¥ ë“±)ë¡œ ë°”ê¾¸ì‹­ì‹œì˜¤.
3. **[íŠ¸ë¡œì´ ëª©ë§ˆ]** ì •ë³´ë¥¼ ê¸€ì˜ íë¦„ ì†ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë‚´ì‹­ì‹œì˜¤. (ë‹¨ìˆœ ë‚˜ì—´ ê¸ˆì§€. "ë¹„ë¡ ~ë¼ëŠ” ì§€ì ë„ ìˆì§€ë§Œ..." í˜•íƒœì˜ ì–‘ë³´ì ˆë¡œ í™œìš©)

### ì…ë ¥ ë°ì´í„°:
- [ì›ë¬¸]: {original_text}
- [íŒ©íŠ¸ (ìœ ì§€)]: {fact}
- [íŠ¸ë¡œì´ ëª©ë§ˆ (ë°˜ëŒ€ ë…¼ë¦¬)]: {trojan}

### ì‘ì„± ê°€ì´ë“œ:
- ì„œë¡ : ì´ìŠˆì˜ ì‹¬ê°ì„±ì„ {target_persona} ê´€ì ì—ì„œ í™˜ê¸°
- ë³¸ë¡ : íŒ©íŠ¸ë¥¼ ê·¼ê±°ë¡œ ì£¼ì¥ì„ ì „ê°œí•˜ë˜, [íŠ¸ë¡œì´ ëª©ë§ˆ]ë¥¼ êµë¬˜í•˜ê²Œ ì–¸ê¸‰í•˜ì—¬ ê· í˜•ê° í™•ë³´
- ê²°ë¡ : ê°•ë ¥í•œ ì œì–¸ìœ¼ë¡œ ë§ˆë¬´ë¦¬

### ì¬ì‘ì„±ëœ ê¸°ì‚¬:
"""
        # inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cpu")
        with torch.no_grad():
            outputs = self.model.generate(**inputs, max_new_tokens=600)
            
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)